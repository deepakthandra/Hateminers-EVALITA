{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....start....cleaning\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score, recall_score, precision_score, classification_report, precision_recall_fscore_support\n",
    "import itertools\n",
    "from string import punctuation\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn import ensemble\n",
    "from sklearn import neural_network\n",
    "from sklearn import linear_model\n",
    "import os \n",
    "import joblib\n",
    "from commen_preprocess import *\n",
    "# from multilabel_data_handler import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_test(pd_test):\n",
    "    comments=pd_test['text'].values\n",
    "    list_comment=[]\n",
    "    for comment in comments:\n",
    "        temp={}\n",
    "        temp['text']=comment\n",
    "        list_comment.append(temp)\n",
    "    return list_comment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Loading Glove Model\n",
      "count10000\n",
      "count20000\n",
      "count30000\n",
      "count40000\n",
      "count50000\n",
      "count60000\n",
      "count70000\n",
      "count80000\n",
      "count90000\n",
      "count100000\n",
      "count110000\n",
      "count120000\n",
      "count130000\n",
      "count140000\n",
      "count150000\n",
      "count160000\n",
      "count170000\n",
      "count180000\n",
      "count190000\n",
      "count200000\n",
      "count210000\n",
      "count220000\n",
      "count230000\n",
      "count240000\n",
      "count250000\n",
      "count260000\n",
      "count270000\n",
      "count280000\n",
      "count290000\n",
      "count300000\n",
      "count310000\n",
      "count320000\n",
      "count330000\n",
      "count340000\n",
      "count350000\n",
      "count360000\n",
      "count370000\n",
      "count380000\n",
      "count390000\n",
      "count400000\n",
      "count410000\n",
      "count420000\n",
      "count430000\n",
      "count440000\n",
      "count450000\n",
      "count460000\n",
      "count470000\n",
      "count480000\n",
      "count490000\n",
      "count500000\n",
      "count510000\n",
      "count520000\n",
      "count530000\n",
      "count540000\n",
      "count550000\n",
      "count560000\n",
      "count570000\n",
      "count580000\n",
      "count590000\n",
      "count600000\n",
      "count610000\n",
      "count620000\n",
      "count630000\n",
      "count640000\n",
      "count650000\n",
      "count660000\n",
      "count670000\n",
      "count680000\n",
      "count690000\n",
      "count700000\n",
      "count710000\n",
      "count720000\n",
      "count730000\n",
      "count740000\n",
      "count750000\n",
      "count760000\n",
      "count770000\n",
      "count780000\n",
      "count790000\n",
      "count800000\n",
      "count810000\n",
      "count820000\n",
      "count830000\n",
      "count840000\n",
      "count850000\n",
      "count860000\n",
      "count870000\n",
      "count880000\n",
      "count890000\n",
      "count900000\n",
      "count910000\n",
      "count920000\n",
      "count930000\n",
      "count940000\n",
      "count950000\n",
      "count960000\n",
      "count970000\n",
      "count980000\n",
      "count990000\n",
      "count1000000\n",
      "count1010000\n",
      "count1020000\n",
      "count1030000\n",
      "count1040000\n",
      "count1050000\n",
      "count1060000\n",
      "count1070000\n",
      "count1080000\n",
      "count1090000\n",
      "count1100000\n",
      "count1110000\n",
      "count1120000\n",
      "count1130000\n",
      "count1140000\n",
      "count1150000\n",
      "count1160000\n",
      "count1170000\n",
      "count1180000\n",
      "count1190000\n",
      "Done. 1193515  words loaded!\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "\n",
    "###give path to glove model file####\n",
    "GLOVE_MODEL_FILE = \"../../LEAM-master/glove.twitter.27B/glove.twitter.27B.200d.txt\"\n",
    "print(os.path.isfile(GLOVE_MODEL_FILE))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding='utf8')\n",
    "    model = {}\n",
    "    i=0\n",
    "    for line in f:\n",
    "        i=i+1\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
    "        model[word] = embedding\n",
    "        if(i%10000==0):\n",
    "            print(\"count\"+str(i))\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 200\n",
    "word2vec_model = loadGloveModel(GLOVE_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### stopwords and punctuations are not removed but text is cleaned and stemmed\n",
    "def glove_tokenize_norem(text):\n",
    "    #text = tokenizer(text)\n",
    "    text=clean(text, remove_stopwords=False, remove_punctuations=False)\n",
    "    words = text.split()\n",
    "    words =[ps.stem(word) for word in words]\n",
    "    return words\n",
    "\n",
    "####stopwords and punctuations are removed along with that text is cleaned ans stemmed\n",
    "def glove_tokenize(text):\n",
    "    #text = tokenizer(text)\n",
    "    text=clean(text, remove_stopwords=False, remove_punctuations=False)\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in STOPWORDS]\n",
    "    words =[ps.stem(word) for word in words]\n",
    "    return words\n",
    "def glove_tokenize_embed(text):\n",
    "    #text = tokenizer(text)\n",
    "    text=clean(text, remove_stopwords=False, remove_punctuations=False)\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in STOPWORDS]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_0:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_0\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_1:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_1\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_10:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_10\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_11:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_11\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_12:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_12\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_13:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_13\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_14:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_14\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_15:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_15\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_16:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_16\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_2:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_2\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_3:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_3\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_4:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_4\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_5:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_5\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_6:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_6\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_7:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_7\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_8:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_8\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_9:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Embeddings_en/sharded_9\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_0/weights:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Encoder_en/DNN/ResidualHidden_0/weights\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_1/weights:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Encoder_en/DNN/ResidualHidden_1/weights\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_2/weights:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Encoder_en/DNN/ResidualHidden_2/weights\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_3/projection:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Encoder_en/DNN/ResidualHidden_3/projection\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_3/weights:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with Encoder_en/DNN/ResidualHidden_3/weights\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/LinearLayer/bias:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with SNLI/Classifier/LinearLayer/bias\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/LinearLayer/weights:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with SNLI/Classifier/LinearLayer/weights\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/tanh_layer_0/bias:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with SNLI/Classifier/tanh_layer_0/bias\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/tanh_layer_0/weights:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with SNLI/Classifier/tanh_layer_0/weights\n",
      "INFO:tensorflow:Initialize variable module/global_step:0 from checkpoint b'/tmp/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47/variables/variables' with global_step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
    "embed = hub.Module(module_url)\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=12,\n",
    "                       allow_soft_placement=True, device_count = {'CPU': 12})\n",
    "\n",
    "def get_embeddings(messages):\n",
    "      \n",
    "    with tf.Session(config=config) as session:\n",
    "            session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "            message_emb = session.run(embed(messages))\n",
    "            \n",
    "    print(\"ending\")\n",
    "    return np.array(message_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "TOKENIZER = glove_tokenize\n",
    "\n",
    "\n",
    "#google encoding used where text is not cleaned \n",
    "def gen_data_google2():\n",
    "    comments = get_data_test(pd_test)\n",
    "    X=[]\n",
    "    for comment in comments:\n",
    "        X.append(clean(comment['text'], remove_stopwords=False, remove_punctuations=False))\n",
    "    #Word Level Features\n",
    "    X =get_embeddings(X)\n",
    "    #y = MultiLabelBinarizer(classes = (1,2,3,4,5,6,7,8,9,10)).fit_transform(y)\n",
    "    return X\n",
    "\n",
    "### tfidf feature generation was used here where stopwords and punctuations are removed \n",
    "def gen_data_new_tfidf():\n",
    "    comments_test=get_data_test(pd_test)\n",
    "    X=[]\n",
    "    for comment in comments_test:\n",
    "        X.append(comment['text'])\n",
    "      \n",
    "    test_word_features = word_vectorizer.transform(X)\n",
    "    test_char_features = char_vectorizer.transform(X)\n",
    "    X = list(hstack([test_char_features, test_word_features]).toarray())\n",
    "    #y = MultiLabelBinarizer(classes = (1,2,3,4,5,6,7,8,9,10)).fit_transform(y)\n",
    "    return X\n",
    "\n",
    "\n",
    "def gen_data_embed():\n",
    "    comments = get_data_test(pd_test)\n",
    "    X=[]\n",
    "    for comment in comments:\n",
    "        words = glove_tokenize(comment['text'].lower())\n",
    "        emb = np.zeros(EMBEDDING_DIM)\n",
    "        for word in words:\n",
    "            try:\n",
    "                emb += word2vec_model[word]\n",
    "            except:\n",
    "                pass\n",
    "        if len(words)!=0:\n",
    "            emb /= len(words)\n",
    "        X.append(emb)\n",
    "        \n",
    "    return X\n",
    "\n",
    "\n",
    "def combine_tf_rem_google_norem_embed():\n",
    "    X=gen_data_google2()\n",
    "    X1=gen_data_new_tfidf()\n",
    "    X2=gen_data_embed()\n",
    "    X=np.concatenate((np.array(X), np.array(X1),np.array(X2)), axis=1)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tfidf_word_vectorizer.pk', 'rb') as fout:\n",
    "         word_vectorizer=pickle.load(fout)\n",
    "\n",
    "with open('tfidf_char_vectorizer.pk', 'rb') as fout:\n",
    "        char_vectorizer=pickle.load(fout)\n",
    "clf_task1=joblib.load('taskA/Catboost_task_1.joblib.pkl')\n",
    "clf_task2=joblib.load('taskB1/Catboost_task_2.joblib.pkl')\n",
    "clf_task3=joblib.load('taskB2/Catboost_task_3.joblib.pkl')\n",
    "select_task1=joblib.load('taskA/Catboost_select_features_task1.joblib.pkl')\n",
    "select_task2=joblib.load('taskB1/Catboost_select_features_task2.joblib.pkl')\n",
    "select_task3=joblib.load('taskB2/Catboost_select_features_task3.joblib.pkl')\n",
    "\n",
    "eng_test_dataset = pd.read_csv('../AMI@EVALITA2018/en_testing.tsv', sep='\\t')\n",
    "pd_test = eng_test_dataset[['id','text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_name= 'pd_train_category'\n",
    "\n",
    "rev_label_map1 = {\n",
    "        0:'discredit',\n",
    "        1:'sexual_harassment',\n",
    "        2:'stereotype',\n",
    "        3:'dominance',\n",
    "        4:'derailing'\n",
    "    }\n",
    "rev_label_map2 = {\n",
    "         1:'active',\n",
    "         0:'passive',\n",
    "     }\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ending\n"
     ]
    }
   ],
   "source": [
    "X=combine_tf_rem_google_norem_embed()\n",
    "list_1,list_2,list_3=[],[],[]\n",
    "for x in X:\n",
    "    x=x.reshape(1,-1)\n",
    "    temp=x\n",
    "    temp=select_task1.transform(temp)\n",
    "    predict1=clf_task1.predict(temp)\n",
    "    list_1.append(int(predict1[0]))\n",
    "    \n",
    "    if(int(predict1[0])!=0):\n",
    "        temp1=x\n",
    "        temp1=select_task2.transform(temp1)\n",
    "        predict2=clf_task2.predict(temp1)\n",
    "        temp2=x\n",
    "        temp2=select_task3.transform(temp2)\n",
    "        predict3=clf_task3.predict(temp2)\n",
    "        list_2.append(rev_label_map1[predict2[0][0]])\n",
    "        list_3.append(rev_label_map2[predict3[0]])\n",
    "    else:\n",
    "        list_2.append(0)\n",
    "        list_3.append(0)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label=eng_test_dataset['TAG'].values\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score,accuracy_score\n",
    "print(classification_report(label,list_1))\n",
    "accuracy_score(label,list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_test = pd_test.assign(d=list_1,e=list_2,f=list_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_test.to_csv('Catboost_all_model.tsv',sep='\\t',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
