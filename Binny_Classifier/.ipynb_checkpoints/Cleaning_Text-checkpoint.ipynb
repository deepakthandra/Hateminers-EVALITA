{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls (vTEXT):\n",
    "    vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\-|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
    "    vTEXT = re.sub(r'(www)(\\w|\\.|\\-|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
    "    return(vTEXT.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_abbre = {\n",
    "\"ain't\" : \"have not\",\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\",\n",
    "\"pls\":\"please\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....start....cleaning\n"
     ]
    }
   ],
   "source": [
    "emoji = {\n",
    "    \"&lt;3\": \" good \",\n",
    "    \":d\": \" good \",\n",
    "    \":dd\": \" good \",\n",
    "    \":p\": \" good \",\n",
    "    \"8)\": \" good \",\n",
    "    \":-)\": \" good \",\n",
    "    \":)\": \" good \",\n",
    "    \";)\": \" good \",\n",
    "    \"(-:\": \" good \",\n",
    "    \"(:\": \" good \",\n",
    "    \"yay!\": \" good \",\n",
    "    \"yay\": \" good \",\n",
    "    \"yaay\": \" good \",\n",
    "    \"yaaay\": \" good \",\n",
    "    \"yaaaay\": \" good \",\n",
    "    \"yaaaaay\": \" good \",\n",
    "    \":/\": \" bad \",\n",
    "    \":&gt;\": \" sad \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" bad \",\n",
    "    \":(\": \" bad \",\n",
    "    \":s\": \" bad \",\n",
    "    \":-s\": \" bad \",\n",
    "    \"&lt;3\": \" heart \",\n",
    "    \":d\": \" smile \",\n",
    "    \":p\": \" smile \",\n",
    "    \":dd\": \" smile \",\n",
    "    \"8)\": \" smile \",\n",
    "    \":-)\": \" smile \",\n",
    "    \":)\": \" smile \",\n",
    "    \";)\": \" smile \",\n",
    "    \"(-:\": \" smile \",\n",
    "    \"(:\": \" smile \",\n",
    "    \":/\": \" worry \",\n",
    "    \":&gt;\": \" angry \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" sad \",\n",
    "    \":(\": \" sad \",\n",
    "    \":s\": \" sad \",\n",
    "    \":-s\": \" sad \",\n",
    "    r\"\\br\\b\": \"are\",\n",
    "    r\"\\bu\\b\": \"you\",\n",
    "    r\"\\bhaha\\b\": \"ha\",\n",
    "    r\"\\bhahaha\\b\": \"ha\"}\n",
    "\n",
    "\n",
    "\n",
    "print(\"....start....cleaning\")\n",
    "\n",
    "\n",
    "\n",
    "#=================stop word====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "stop_words = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to']\n",
    "\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "\n",
    "#=================replace the duplicate word=====================\n",
    "import re\n",
    "\n",
    "def substitute_repeats_fixed_len(text, nchars, ntimes=3):\n",
    "    \"\"\"\n",
    "         Find substrings that consist of `nchars` non-space characters\n",
    "         and that are repeated at least `ntimes` consecutive times,\n",
    "         and replace them with a single occurrence.\n",
    "         Examples: \n",
    "         abbcccddddeeeee -> abcde (nchars = 1, ntimes = 2)\n",
    "         abbcccddddeeeee -> abbcde (nchars = 1, ntimes = 3)\n",
    "         abababcccababab -> abcccab (nchars = 2, ntimes = 2)\n",
    "    \"\"\"\n",
    "    return re.sub(r\"(\\S{{{}}})(\\1{{{},}})\".format(nchars, ntimes-1), r\"\\1\", text)\n",
    "\n",
    "def substitute_repeats(text, ntimes=3):\n",
    "        # Truncate consecutive repeats of short strings\n",
    "        for nchars in range(1, 20):\n",
    "            text = substitute_repeats_fixed_len(text, nchars, ntimes)\n",
    "        return text\n",
    "\n",
    "\n",
    "\n",
    "#=================choose one of tokenizer=======================\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer=TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================final clean function=======================\n",
    "def clean(comment, remove_stopwords=True, remove_punctuations=False):\n",
    "    #comment=re.sub(\"\\.\\.\",\" .\",comment)\n",
    "    comment=remove_urls(comment.lower())\n",
    "    #remove \\n\n",
    "    comment=re.sub(r\"\\t\",\" \",comment)\n",
    "    comment=re.sub(r\"\\r\\n\",\" . \",comment)\n",
    "    comment=re.sub(r\"\\r\",\" . \",comment)\n",
    "    comment=re.sub(r\"\\n\",\" . \",comment)\n",
    "    comment=re.sub(r\"\\\\n\\n\",\" . \",comment)\n",
    "    comment=re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",comment)\n",
    "    comment=re.sub(\"\\[\\[.*\\]\",\"\",comment)\n",
    "    comment = re.sub(r\"\\'ve\", \" have \", comment)\n",
    "    \n",
    "    comment = re.sub(r\"\\'d\", \" would \", comment)\n",
    "    comment = re.sub(r\"\\'ll\", \" will \", comment)\n",
    "    comment = re.sub(r\"ca not\", \"cannot\", comment)\n",
    "    comment = re.sub(r\"you ' re\", \"you are\", comment)\n",
    "    comment = re.sub(r\"wtf\",\"what the fuck\", comment)\n",
    "    comment = re.sub(r\"i ' m\", \"I am\", comment)\n",
    "    comment = re.sub(r\"I\", \"one\", comment)\n",
    "    comment = re.sub(r\"II\", \"two\", comment)\n",
    "    comment = re.sub(r\"III\", \"three\", comment)\n",
    "    comment=re.sub(r\"mothjer\",\"mother\",comment)\n",
    "    comment=re.sub(r\"nazi\",\"nazy\",comment)\n",
    "    comment=re.sub(r\"withought\",\"with out\",comment)\n",
    "    comment=substitute_repeats(comment)\n",
    "    s=comment\n",
    "    \n",
    "\n",
    "    \n",
    "    s = s.replace('&', '')\n",
    "    s = s.replace('@', '')\n",
    "    s = s.replace('0', '')\n",
    "    s = s.replace('1', '')\n",
    "    s = s.replace('2', '')\n",
    "    s = s.replace('3', '')\n",
    "    s = s.replace('4', '')\n",
    "    s = s.replace('5', '')\n",
    "    s = s.replace('6', '')\n",
    "    s = s.replace('7', '')\n",
    "    s = s.replace('8', '')\n",
    "    s = s.replace('9', '')\n",
    "    # s = s.replace('雲水','')\n",
    "\n",
    "    comment=s\n",
    "    #comment = re_tok.sub(' ', comment)\n",
    "    #print(comment)\n",
    "    words=tokenizer.tokenize(comment)\n",
    "    words=[no_abbre[word] if word in no_abbre else word for word in words]\n",
    "    words=[emoji[word] if word in emoji else word for word in words]\n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    sent=\" \".join(words)\n",
    "    # Remove some special characters, or noise charater, but do not remove all!!\n",
    "    if remove_punctuations:\n",
    "        sent = re.sub(r'([\\'\\\"\\/\\-\\_\\--\\_])',' ', sent)\n",
    "    else:\n",
    "        sent = re.sub(r'([\\'\\\"\\/\\-\\_\\-\\_\\(\\)\\{\\}])',' ', sent)\n",
    "    clean_sent= re.sub(r'([\\;\\|•«\\n])',' ', sent)\n",
    "    clean_sent = re.sub(r\"n't\", \" not \", clean_sent)\n",
    "    \n",
    "    FLAG_remove_non_ascii =True\n",
    "    if FLAG_remove_non_ascii:\n",
    "        return clean_sent.encode(\"ascii\", errors=\"ignore\").decode().strip()\n",
    "    else:\n",
    "        return clean_sent.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(pd_train):\n",
    "    comments=pd_train['text'].values\n",
    "    list_comment=[]\n",
    "    for comment in comments:\n",
    "        temp={}\n",
    "        temp['text']=comment\n",
    "        list_comment.append(temp)\n",
    "    return list_comment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_train_dataset = pd.read_csv('../AMI@EVALITA2018/en_testing.tsv', sep='\\t')\n",
    "#pd_train_binary = eng_train_dataset[['id','misogynous','text','misogyny_category','target']]\n",
    "pd_train_binary = eng_train_dataset[['id','text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages (3.6.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages (from tweepy) (1.11.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages (from tweepy) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.11.1 in /home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages (from tweepy) (2.18.4)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in /home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages (from tweepy) (1.6.7)\n",
      "Requirement already satisfied: oauthlib>=0.6.2 in /home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages (from requests-oauthlib>=0.7.0->tweepy) (2.1.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages (from requests>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages (from requests>=2.11.1->tweepy) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages (from requests>=2.11.1->tweepy) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages (from requests>=2.11.1->tweepy) (2018.4.16)\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy\n",
    "import tweepy\n",
    "from pprint import pprint\n",
    "from secrets import consumer_key, consumer_secret,access_token, access_token_secret\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from collections import Counter\n",
    "import sys\n",
    "\n",
    "#authorisation with twitter give own when using \n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:05<00:00, 775.87it/s]\n"
     ]
    }
   ],
   "source": [
    "import preprocessor as p\n",
    "import sys\n",
    "\n",
    "orig_stdout = sys.stdout\n",
    "f= open(\"all_cleaned_test.txt\",\"w+\")\n",
    "sys.stdout = f\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(pd_train_binary))):\n",
    "        comment=pd_train_binary.iloc[[i][:]]\n",
    "\n",
    "        print(\"UNCLEANED_TEXT:\",comment['text'][i])\n",
    "        print(\"CLEANED_TEXT:\",clean(comment['text'][i], remove_stopwords=False, remove_punctuations=False))\n",
    "        print(\"CLEANED_TEXT_PREPROCESSOR:\",p.clean(comment['text'][i]))\n",
    "        l=p.parse(comment['text'][i])\n",
    "        print(\"ALL_mention:\",l.mentions)\n",
    "        print(\"ALL_url:\",l.urls)\n",
    "        print(\"ALL_hastags:\",l.hashtags)\n",
    "        print(\"ALL_emojis:\",l.emojis)\n",
    "        print(\"ALL_smiley:\",l.smileys)\n",
    "        print(\"ALL_number:\",l.numbers)\n",
    "#         print(\"LABEL:\",comment['misogynous'][i])\n",
    "#         print(\"CATEGORY:\",comment['misogyny_category'][i])\n",
    "#         print(\"TARGET:\",comment['target'][i])\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        \n",
    "        \n",
    "sys.stdout = orig_stdout\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for f in female_count:\n",
    "    if(f>0):\n",
    "        count=count+1\n",
    "\n",
    "\n",
    "print(count)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train_binary = pd_train_binary.assign(male=male_count,female=female_count)\n",
    "pd_train_binary.to_csv('../AMI@EVALITA2018/my_pd_train.tsv',sep='\\t',index=False)\n",
    "\n",
    "#name=parsed_tweet.mentions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Press', 'Start', 'Kofi']\n",
      "['Press', 'Start', 'Kofi']\n",
      "unknown\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:punyajoy-nogpu]",
   "language": "python",
   "name": "conda-env-punyajoy-nogpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
